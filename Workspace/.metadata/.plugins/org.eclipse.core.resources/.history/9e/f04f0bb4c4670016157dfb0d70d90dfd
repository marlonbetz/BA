from pipeline import *

print("LOAD TEST WORDLIST")
pathToAnnotatedWordList = "Data/IELex/output/IELex-2016.tsv.asjp"

languages,words,global_ids,cognate_classes = loadAnnotatedWordList(pathToAnnotatedWordList, {1494})

print("VECTORIZE TEST WORDS")
padToMaxLength=15
bpf = BinaryPhonemeFeatures()
X = np.array([bpf.encodeWord(word, padToMaxLength=padToMaxLength).flatten() for word in words])
print("shape of X:",X.shape)

print("FIT VAE")

batch_size = X.shape[0]
dim_phoneme_embeddings = 16
original_dim = dim_phoneme_embeddings * padToMaxLength
latent_dim = 2
intermediate_dim = 100


epsilon_std = 0.01
nb_epoch =0

vae = VAE(latent_dim=latent_dim,
          original_dim=original_dim,
          intermediate_dim=intermediate_dim,
          batch_size=batch_size,
          epsilon_std=epsilon_std)

vae.fit(X,
      nb_epoch=nb_epoch)


print("EMBED TEST WORDS")
embeddings = vae.embed(X)

print("CLUSTER WORDS OF EVERY CONCEPT")
concepts2embeddings = dict((concept,[emb for i,emb in enumerate(embeddings) if global_ids[i] == concept]) for concept in set(sorted(global_ids)))
concepts2cognate_classes = dict((concept,[cog for i,cog in enumerate(cognate_classes) if global_ids[i] == concept]) for concept in set(sorted(global_ids)))
#for damping_factor in np.arange(0.5,1,0.05):
damping_factor = 0.5
ap = AffinityPropagation(damping=damping_factor)
n_cognate_classes = len(set(cognate_classes))
n_concepts = len(set(global_ids))
y_true = cognate_classes
y_pred = ap.fit_predict(embeddings)
y_random = np.random.randint(0,int(n_cognate_classes/n_concepts),y_pred.shape)
print(metrics.adjusted_rand_score(y_true, y_pred))
print(metrics.adjusted_mutual_info_score(y_true, y_pred))
print(metrics.adjusted_rand_score(y_true, y_random))
print(metrics.adjusted_mutual_info_score(y_true, y_random))


  
print("PLOTTING")
import matplotlib.pyplot as plt
import seaborn as sns
prior = np.random.normal(0,1,(1000,2))
    
plt.subplot(1,2,1)
sns.kdeplot(prior[:,0],prior[:,1])
plt.subplot(1,2,2)
sns.kdeplot(prior[:,0],prior[:,1])


sns.set_style("white")
cmap_y_pred = dict((label,np.random.beta(1,1,3)) for label in y_pred)
cmap_y_true = dict((label,np.random.beta(1,1,3)) for label in y_true)
for word,emb,y_p,y_t in zip(words,embeddings,y_pred,y_true):
    plt.subplot(1,2,1)
    plt.annotate(word,emb,color=cmap_y_true[y_t])
    plt.subplot(1,2,2)
    plt.annotate(word,emb,color=cmap_y_pred[y_p])
plt.show()
