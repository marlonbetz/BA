from pipeline import *

print("LOAD TEST WORDLIST")
pathToAnnotatedWordList = "Data/IELex/output/IELex-2016.tsv.asjp"

languages,words,global_ids,cognate_classes = loadAnnotatedWordList(pathToAnnotatedWordList)

print("VECTORIZE TEST WORDS")
padToMaxLength=15
bpf = BinaryPhonemeFeatures()

X_dict = dict((concept,np.array([bpf.encodeWord(word, padToMaxLength=padToMaxLength).flatten() for word,concept_tmp in zip(words,global_ids) if  concept_tmp == concept]))for concept in global_ids)
concepts2cognate_classes = dict((concept,[cog for i,cog in enumerate(cognate_classes) if global_ids[i] == concept]) for concept in set(sorted(global_ids)))

adjusted_rand_scores =  np.zeros((len(concepts2cognate_classes)))
adjusted_mutual_info_scores  =[]
homogeneity_completeness_v_measures_scores = []
n_cognate_classes_true_perDataPoint = []
n_cognate_classes_pred_perDataPoint = []
n_concepts = len(X_dict.keys())

n_ensemble = 4
for i,item in enumerate(X_dict.items()):
    ars_tmp = []
    ami_tmp = []
    hcv_tmp = []
    for n in range(n_ensemble):
        print(i,"/",n_concepts,n,"/",n_ensemble)
        concept,X  = item
        print("FIT VAE")
        
        batch_size = X.shape[0]
        dim_phoneme_embeddings = 16
        original_dim = dim_phoneme_embeddings * padToMaxLength
        latent_dim = 20
        intermediate_dim = 500
        
        
        epsilon_std = 0.01
        nb_epoch =40000
        
        vae = VAE(latent_dim=latent_dim,
                  original_dim=original_dim,
                  intermediate_dim=intermediate_dim,
                  batch_size=batch_size,
                  epsilon_std=epsilon_std)
        
        vae.fit(X,
              nb_epoch=nb_epoch)
        
        
        print("EMBED TEST WORDS")
        embeddings = vae.embed(X)
        
        print("CLUSTER WORDS")
        #for damping_factor in np.arange(0.5,1,0.05):
    
        damping_factor = 0.5
        ap = AffinityPropagation(damping=damping_factor)
        n_cognate_classes = len(set(cognate_classes))
        n_concepts = len(set(global_ids))
        y_true = concepts2cognate_classes[concept]
        y_pred = ap.fit_predict(embeddings)
        y_random = np.random.randint(0,int(n_cognate_classes/n_concepts),y_pred.shape)
        
        ars = metrics.adjusted_rand_score(y_true, y_pred)
        ami = metrics.adjusted_mutual_info_score(y_true, y_pred)
        hcv = metrics.homogeneity_completeness_v_measure(y_true, y_pred)
        ars_tmp.append(ars)
        ami_tmp.append(ami)
        hcv_tmp.append(hcv)
        
        
        
        print(ars)
        print(ami)
        print(hcv)
        
        print(metrics.adjusted_rand_score(y_true, y_random))
        print(metrics.adjusted_mutual_info_score(y_true, y_random))
        print(metrics.homogeneity_completeness_v_measure(y_true, y_random))
        
    adjusted_rand_scores.append(ars_tmp)
    adjusted_mutual_info_scores.append(ami_tmp)
    homogeneity_completeness_v_measures_scores.append(hcv_tmp)
    
    n_cognate_classes_true_tmp =  len(set(concepts2cognate_classes[concept]))
    n_cognate_classes_pred_tmp =  len(set(y_pred))
    print("estimated number of cognate classes",n_cognate_classes_pred_tmp)
    print("true number of cognate classes",n_cognate_classes_true_tmp)
    n_cognate_classes_true_perDataPoint.append(n_cognate_classes_true_tmp)
    n_cognate_classes_pred_perDataPoint.append(n_cognate_classes_pred_tmp)

print("adjusted_rand_scores",np.mean(adjusted_rand_scores))
print("adjusted_mutual_info_scores",np.mean(adjusted_mutual_info_scores))
print("homogeneity_completeness_v_measures_scores",np.mean(np.array(homogeneity_completeness_v_measures_scores),axis=0))
import pickle 
import codecs
pickle.dump(adjusted_rand_scores,codecs.open("adjusted_rand_scores.pkl","wb"))
pickle.dump(adjusted_mutual_info_scores,codecs.open("adjusted_mutual_info_scores.pkl","wb"))
pickle.dump(homogeneity_completeness_v_measures_scores,codecs.open("homogeneity_completeness_v_measures_scores.pkl","wb"))
pickle.dump(n_cognate_classes_true_perDataPoint,codecs.open("n_cognate_classes_true_perDataPoint.pkl","wb"))
pickle.dump(n_cognate_classes_pred_perDataPoint,codecs.open("n_cognate_classes_pred_perDataPoint.pkl","wb"))
