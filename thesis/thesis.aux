\relax 
\citation{bouckaert2012mapping}
\citation{gray2009language}
\citation{dellert2015uralic}
\citation{bouchard2013automated}
\citation{rama2016siamese}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{3}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Sound Change as a Walk in Latent Space}{4}}
\newlabel{Sound Change as a Walk in Latent Space}{{2}{4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Motivation}{4}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Visualization of the concept of sound change as a walk in latent space. Here, sound changes are vectors from one word to another, where both word forms are given as points in latent space. The vector from /lob/ to /lop/ would be the same as from /tag/ to /tak/, as both vectors would describe the loss of voice in the word-final phoneme. This linear dependence means that if we fit a regression model from /tag/ to /tak/, we could generalize well to predict /lop/ from /lob/. The different lengths of the vectors are than proportional to probabilities of such as sound change to appear. Here, final devoicing should be more probable than a change from /lop/ to /tak/. \relax }}{4}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{default}{{1}{4}}
\citation{bouchard2007probabilistic}
\citation{bouchard2013automated}
\citation{bouchard2007probabilistic}
\newlabel{eq:final_devoicing}{{3}{5}}
\newlabel{eq:sound_change_linear_dependency}{{4}{5}}
\citation{mikolov2013distributed}
\citation{kondrak2000new}
\citation{list2012sca}
\citation{rama2016siamese}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}$P(z)$ as Prior for Evolutionary Stable Phoneme sequences}{6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Sound Change as Posterior $P(w_{recent}|w_{ancient},z)$ }{7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4}Cognate Identification as Hypothesis Comparison}{8}}
\citation{bouchard2007probabilistic}
\citation{bouchard2013automated}
\citation{Goodfellow-et-al-2016-Book}
\citation{chomsky1968sound}
\citation{kondrak2000new}
\citation{rama2016siamese}
\citation{rama2016siamese}
\citation{rama2016siamese}
\@writefile{toc}{\contentsline {section}{\numberline {3}Related Research}{9}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Architecture}{9}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Phoneme Vectorization}{9}}
\newlabel{Phoneme Vectorization}{{4.1}{9}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.1}Hand-crafted Vectorization Models}{9}}
\citation{landauer2013handbook}
\citation{mikolov2013efficient}
\citation{mikolov2013distributed}
\citation{goldberg2014word2vec}
\citation{pennington2014glove}
\citation{morin2005hierarchical}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.2}Data-driven Embeddings}{10}}
\newlabel{eq:hierarchicalsoftmax_nodeprob}{{16}{10}}
\citation{goldberg2014word2vec}
\citation{gutmann2010noise}
\citation{mnih2012fast}
\citation{jager2014phylogenetic}
\citation{rama2016siamese}
\newlabel{eq:neg_sampling_3}{{17}{11}}
\citation{mikolov2013efficient}
\citation{mikolov2013efficient}
\citation{doersch2016tutorial}
\citation{doersch2016tutorial}
\citation{kingma2013auto}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces The two common architectures of word2vec. The Continuous Bag-of-Words (CBOW) model predicts the current word based on the context, and the Skip-gram predicts surrounding words given the current word. From \cite  {mikolov2013efficient}.\relax }}{12}}
\newlabel{fig:word2vec}{{2}{12}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Word Embeddings}{12}}
\newlabel{Word Embeddings}{{4.2}{12}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.1}Autoencoders}{12}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces t-SNE visualization of the embeddings of all 720 phonemes contained in ASJP, trained with word2vec. The colors represent clusters inferred by Affinity Propagation. The model can clearly separate vowels and various forms of special articulation types. Pulmonic consonants can also be separated clearly from other coarticulation variants, as can uvular or pre-nasalized sounds. Less frequent phonemes, however, cannot be clearly differentiated. \relax }}{13}}
\newlabel{fig:phoneme_embeddings_overview_ap}{{3}{13}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.2}Variational Autoencoders}{13}}
\newlabel{Variational Autoencoders}{{4.2.2}{13}}
\citation{kingma2013auto}
\citation{kingma2013auto}
\citation{kingma2013auto}
\newlabel{eq:vae_loss_original}{{23}{14}}
\newlabel{eq:vae_loss_reparameterized}{{27}{14}}
\citation{rehurek_lrec}
\citation{chollet2015keras}
\citation{tensorflow2015-whitepaper}
\citation{scikit-learn}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Clustering}{15}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.3.1}Affinity Propagation}{15}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Evaluation}{15}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Data}{15}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Evaluation of $P(X)$}{15}}
\bibstyle{apa}
\bibdata{references}
\bibcite{tensorflow2015-whitepaper}{\astroncite {Abadi et\nobreakspace  {}al.}{2015}}
\bibcite{Goodfellow-et-al-2016-Book}{\astroncite {Bengio and Courville}{2016}}
\bibcite{bouchard2013automated}{\astroncite {Bouchard-C{\^o}t{\'e} et\nobreakspace  {}al.}{2013}}
\bibcite{bouchard2007probabilistic}{\astroncite {Bouchard-C{\^o}t{\'e} et\nobreakspace  {}al.}{2007}}
\bibcite{bouckaert2012mapping}{\astroncite {Bouckaert et\nobreakspace  {}al.}{2012}}
\bibcite{chollet2015keras}{\astroncite {Chollet}{2015}}
\bibcite{chomsky1968sound}{\astroncite {Chomsky and Halle}{1968}}
\bibcite{dellert2015uralic}{\astroncite {Dellert}{2015}}
\bibcite{doersch2016tutorial}{\astroncite {Doersch}{2016}}
\bibcite{goldberg2014word2vec}{\astroncite {Goldberg and Levy}{2014}}
\bibcite{gray2009language}{\astroncite {Gray et\nobreakspace  {}al.}{2009}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3}Results}{16}}
\@writefile{toc}{\contentsline {section}{\numberline {6}Resume}{16}}
\@writefile{toc}{\contentsline {section}{\numberline {7}Acknowledgements}{16}}
\bibcite{gutmann2010noise}{\astroncite {Gutmann and Hyv{\"a}rinen}{2010}}
\bibcite{jager2014phylogenetic}{\astroncite {J{\"a}ger}{2014}}
\bibcite{kingma2013auto}{\astroncite {Kingma and Welling}{2013}}
\bibcite{kondrak2000new}{\astroncite {Kondrak}{2000}}
\bibcite{landauer2013handbook}{\astroncite {Landauer et\nobreakspace  {}al.}{2013}}
\bibcite{list2012sca}{\astroncite {List}{2012}}
\bibcite{mikolov2013efficient}{\astroncite {Mikolov et\nobreakspace  {}al.}{2013a}}
\bibcite{mikolov2013distributed}{\astroncite {Mikolov et\nobreakspace  {}al.}{2013b}}
\bibcite{mnih2012fast}{\astroncite {Mnih and Teh}{2012}}
\bibcite{morin2005hierarchical}{\astroncite {Morin and Bengio}{2005}}
\bibcite{scikit-learn}{\astroncite {Pedregosa et\nobreakspace  {}al.}{2011}}
\bibcite{pennington2014glove}{\astroncite {Pennington et\nobreakspace  {}al.}{2014}}
\bibcite{rama2016siamese}{\astroncite {Rama}{2016}}
\bibcite{rehurek_lrec}{\astroncite {{\v R}eh{\r u}{\v r}ek and Sojka}{2010}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Other t-SNE visualizations of the embeddings created by word2vec. (top left) The model learns to clearly separate natural classes such as vowels, plain pulmonic or glottalized consonants, while other articulations seem to spread over the feature space. The colors indicate membership of a natural phonological class. (top right) A more detailed view on plain pulmonic consonants. Note the linear dependencies between voiced and unvoiced plosives and their respective nasal variant. (bottom left) Another detailed view. Note how the labialized uvular sounds cluster among glottalized consonants. (top right) The model seems to capture different manners of articulations across articulation type boundaries, as the linear dependency shows here.\relax }}{18}}
\newlabel{fig:phoneme_embeddings_word2vec_all}{{4}{18}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Comparison of the four word2vec models evaluated. (top left) Both CBOW Models perform better than the skip-gram models over all numbers of embedding dimension. (bottom left) Mean distance between the predicted vector and the target with regard to embedding dimensions. Here, negative sampling yields less error than hierarchical softmax. (top right) Mean accuracy for the models with regard to the context window size. The models perform worse the bigger the context is. (bottom right) Mean distance between the predicted vector and the target with regard to context window size. Again, bigger contexts lead to worse predictions. \relax }}{19}}
\newlabel{fig:phoneme_embeddings_4models_comparison_meanAccLossTopn1}{{5}{19}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Mean accuracies for analogies. The model does not seem to be able to capture the compositionality of the latent features too well. (top left) Accuracy is best for plosives across plain pulmonic as well as more complex articulations, while fricatives perform worse. (top right) Glottalized and Plain pulmonic consonant phonemes yield best performance. Among complex articulations, which all perform bad, labialized phonemes yield best results, while aspirated and palatalized phonemes perform even worse. (bottom left) Adding voice works better than removing it or adding nasality. (bottom right) Vowel analogies work far better than consonant analogies. This should be due to the small number of possible vowel phonemes.\relax }}{20}}
\newlabel{fig:phoneme_embeddings_detailed_evaluation_topn1}{{6}{20}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Mean accuracies for analogies over the pooled consonant phonemes. Here, the model is expected to yield better results, as the number of latent features should have been reduced. (left) Accuracy is doubled compared with the unpooled phonemes. (middle) Adding voice works quite well, while removing it still yields acceptable performance. Applying nasality again works quite bad. (right) Vowel analogy tasks seem to work reasonably well, but worse than with unpooled consonants.\relax }}{20}}
\newlabel{fig:phoneme_embeddings_detailed_evaluation_topn1_poooled}{{7}{20}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Visualization of the Variational autoencoder architecture. (left) The model with original objective as in Eq. 23\hbox {}. The stochastic unit is inside the network and would not allow for the backpropagation of error gradients through the network. (right) The model after the reparameterization with the objective as in Eq. 27\hbox {}. Here, the sampling is interpreted as an input variable, so error gradients can be backpropagated through the whole model. From \cite  {doersch2016tutorial}.\relax }}{21}}
\newlabel{fig:vae_model_visualization}{{8}{21}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces A detailed view on a subsample of ASJP embedded into $\mathcal  {Z}$ after trained on the whole ASJP data set for 100 iterations. As can be seen, words accumulate in local subspaces with higher probability mass. Here, all words are contained in an area with shorter words that show simple CV syllable structures. Words that only differ in one distinctive feature are very close to each other, while linear dependencies over longer distances signify relationships over whole syllables that are exchanged.\relax }}{22}}
\newlabel{fig:vae_phono_binary_asjp_subsample_words}{{9}{22}}
\newlabel{fig:vae_phono_binary_wordLength0}{{\caption@xref {fig:vae_phono_binary_wordLength0}{ on input line 567}}{23}}
\newlabel{sub@fig:vae_phono_binary_wordLength0}{{}{23}}
\newlabel{fig:vae_phono_binary_wordLength1}{{\caption@xref {fig:vae_phono_binary_wordLength1}{ on input line 573}}{23}}
\newlabel{sub@fig:vae_phono_binary_wordLength1}{{}{23}}
\newlabel{fig:vae_phono_binary_wordLength2}{{\caption@xref {fig:vae_phono_binary_wordLength2}{ on input line 579}}{23}}
\newlabel{sub@fig:vae_phono_binary_wordLength2}{{}{23}}
\newlabel{fig:vae_phono_binary_wordLength3}{{\caption@xref {fig:vae_phono_binary_wordLength3}{ on input line 584}}{23}}
\newlabel{sub@fig:vae_phono_binary_wordLength3}{{}{23}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces The ASJP data set embedded into $\mathcal  {Z}$ after trained on it for 100 iterations, colored by word length. The model learns to cluster words according to their respective length, with more frequent word lengths located in subspaces with higher probability mass.\relax }}{23}}
\newlabel{fig:vae_phono_binary_wordLength}{{10}{23}}
\newlabel{fig:vae_phono_binary_features0}{{\caption@xref {fig:vae_phono_binary_features0}{ on input line 594}}{24}}
\newlabel{sub@fig:vae_phono_binary_features0}{{}{24}}
\newlabel{fig:vae_phono_binary_features1}{{\caption@xref {fig:vae_phono_binary_features1}{ on input line 600}}{24}}
\newlabel{sub@fig:vae_phono_binary_features1}{{}{24}}
\newlabel{fig:vae_phono_binary_features2}{{\caption@xref {fig:vae_phono_binary_features2}{ on input line 606}}{24}}
\newlabel{sub@fig:vae_phono_binary_features2}{{}{24}}
\newlabel{fig:vae_phono_binary_feature3}{{\caption@xref {fig:vae_phono_binary_feature3}{ on input line 611}}{24}}
\newlabel{sub@fig:vae_phono_binary_feature3}{{}{24}}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces The ASJP data set embedded into $\mathcal  {Z}$ after trained on it for 100 iterations, colored by whether a given word has a phoneme with some specific distinctive feature. (top left) The model learns that clicks are highly unlikely to emerge evolutionary and hence assigns low probability mass to their respective subspace. (top right) The distribution of words with and without dentals. The linear dependencies are clearly visible. (bottom left) The distribution of words with and without fricatives. Again, it can be seen how they are linearly dependent of each other. (bottom right) A detailed view on the words with and without fricatives.\relax }}{24}}
\newlabel{fig:vae_phono_binary_features}{{11}{24}}
\newlabel{fig:vae_phono_orthofeatures0}{{\caption@xref {fig:vae_phono_orthofeatures0}{ on input line 621}}{25}}
\newlabel{sub@fig:vae_phono_orthofeatures0}{{}{25}}
\newlabel{fig:vae_phono_orthofeatures1}{{\caption@xref {fig:vae_phono_orthofeatures1}{ on input line 627}}{25}}
\newlabel{sub@fig:vae_phono_orthofeatures1}{{}{25}}
\newlabel{fig:vae_phono_orthofeatures2}{{\caption@xref {fig:vae_phono_orthofeatures2}{ on input line 633}}{25}}
\newlabel{sub@fig:vae_phono_orthofeatures2}{{}{25}}
\newlabel{fig:vae_phono_orthofeatures3}{{\caption@xref {fig:vae_phono_orthofeatures3}{ on input line 638}}{25}}
\newlabel{sub@fig:vae_phono_orthofeatures3}{{}{25}}
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces The ASJP data set embedded into $\mathcal  {Z}$ after trained on it for 100 iterations. Since the binary features ignore coarticulations as such and encode them as independent phonemes instead, the model might learn to distinguish them, given some underlying latent features. (top left) The distribution of words that contain plain /w/ versus words with proper labialization. If the model did not learn the difference, we would expect some uniform distributions over all words that contain /w/ in the input. However, we can clearly see that words with proper labialization are located in other subspaces than words with proper /w/. Moreover, we see that words with labialized phonemes show linear dependence. (top right) The distribution of words that contain plain /y/ versus words with proper palatalization. Again, we see a clear distinction between the two. (bottom left) The distribution of words that contain plain /h/ versus words with proper aspiration. Again, we see a clear distinction between the two. (bottom right) The distribution of words that contain glottalized sounds versus that of words without. As glottalization can cover both vowels and consonants, the distributions are spread over the whole population. However, we can see that glottalized sounds cluster at the bottom right, a subspace containing mostly mostly words with velar and uvular consonants.\relax }}{25}}
\newlabel{fig:vae_phono_orthofeatures}{{12}{25}}
