\relax 
\citation{bouckaert2012mapping}
\citation{gray2009language}
\citation{dellert2015uralic}
\citation{bouchard2013automated}
\citation{rama2016siamese}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{3}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Sound Change as a Walk in Latent Space}{4}}
\newlabel{Sound Change as a Walk in Latent Space}{{2}{4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Motivation}{4}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Visualization of the concept of sound change as a walk in latent space. Here, sound changes are vectors from one word to another, where both word forms are given as points in latent space. The vector from /lob/ to /lop/ would be the same as from /tag/ to /tak/, as both vectors would describe the loss of voice in the word-final phoneme. This linear dependence means that if we fit a regression model from /tag/ to /tak/, we could generalize well to predict /lop/ from /lob/. The different lengths of the vectors are than proportional to probabilities of such as sound change to appear. Here, final devoicing should be more probable than a change from /lop/ to /tak/. }}{4}}
\newlabel{default}{{1}{4}}
\citation{bouchard2007probabilistic}
\citation{bouchard2013automated}
\citation{bouchard2007probabilistic}
\citation{mikolov2013distributed}
\newlabel{eq:final_devoicing}{{3}{5}}
\newlabel{eq:sound_change_linear_dependency}{{4}{5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}$P(z|X)$ as Prior for Evolutionary Stable Word Forms}{5}}
\citation{kondrak2000new}
\citation{list2012sca}
\citation{rama2016siamese}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Sound Change as Posterior $P(w_{recent}|w_{ancient},z)$ }{6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4}Cognate Identification as Hypothesis Comparison}{7}}
\citation{landauer2013handbook}
\citation{mikolov2013efficient}
\citation{mikolov2013distributed}
\citation{goldberg2014word2vec}
\citation{pennington2014glove}
\@writefile{toc}{\contentsline {section}{\numberline {3}Related Research}{8}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Architecture}{8}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Phoneme Vectorization}{8}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.1}Hand-crafted Vectorization Models}{8}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.2}Data-driven Embeddings}{8}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces t-SNE visualization of the embeddings of all 720 phonemes contained in ASJP, trained with word2vec. The colors represent clusters inferred by Affinity Propagation. The model can clearly separate vowels and various forms of special articulation types. Pulmonic consonants can also be separated clearly from other coarticulation variants, as can uvular or pre-nasalized sounds. Less frequent phonemes, however, cannot be clearly differentiated. }}{9}}
\newlabel{default}{{2}{9}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Word Embeddings}{9}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.1}Autoencoders}{9}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.2}Variational Autoencoders}{9}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Other t-SNE visualizations of the embeddings created by word2vec. (top left) The model learns to clearly separate natural classes such as vowels, plain pulmonic or glottalized consonants, while other articulations seem to spread over the feature space. The colors indicate membership of a natural phonological class. (top right) A more detailed view on plain pulmonic consonants. Note the linear dependencies between voiced and unvoiced plosives and their respective nasal variant. (bottom left) Another detailed view. Note how the labialized uvular sounds cluster among glottalized consonants. (top right) The model seems to capture different manners of articulations across articulation type boundaries, as the linear dependency shows here.}}{10}}
\newlabel{fig:word2vec_all}{{3}{10}}
\citation{rehurek_lrec}
\citation{chollet2015keras}
\citation{tensorflow2015-whitepaper}
\citation{scikit-learn}
\bibstyle{apa}
\bibdata{references}
\bibcite{tensorflow2015-whitepaper}{\astroncite {Abadi et\nobreakspace  {}al.}{2015}}
\bibcite{bouchard2013automated}{\astroncite {Bouchard-C{\^o}t{\'e} et\nobreakspace  {}al.}{2013}}
\bibcite{bouchard2007probabilistic}{\astroncite {Bouchard-C{\^o}t{\'e} et\nobreakspace  {}al.}{2007}}
\bibcite{bouckaert2012mapping}{\astroncite {Bouckaert et\nobreakspace  {}al.}{2012}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Clustering}{11}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.3.1}Affinity Propagation}{11}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Evaluation}{11}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Data}{11}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Results}{11}}
\@writefile{toc}{\contentsline {section}{\numberline {6}Resume}{11}}
\@writefile{toc}{\contentsline {section}{\numberline {7}Acknowledgements}{11}}
\bibcite{chollet2015keras}{\astroncite {Chollet}{2015}}
\bibcite{dellert2015uralic}{\astroncite {Dellert}{2015}}
\bibcite{goldberg2014word2vec}{\astroncite {Goldberg and Levy}{2014}}
\bibcite{gray2009language}{\astroncite {Gray et\nobreakspace  {}al.}{2009}}
\bibcite{kondrak2000new}{\astroncite {Kondrak}{2000}}
\bibcite{landauer2013handbook}{\astroncite {Landauer et\nobreakspace  {}al.}{2013}}
\bibcite{list2012sca}{\astroncite {List}{2012}}
\bibcite{mikolov2013efficient}{\astroncite {Mikolov et\nobreakspace  {}al.}{2013a}}
\bibcite{mikolov2013distributed}{\astroncite {Mikolov et\nobreakspace  {}al.}{2013b}}
\bibcite{scikit-learn}{\astroncite {Pedregosa et\nobreakspace  {}al.}{2011}}
\bibcite{pennington2014glove}{\astroncite {Pennington et\nobreakspace  {}al.}{2014}}
\bibcite{rama2016siamese}{\astroncite {Rama}{2016}}
\bibcite{rehurek_lrec}{\astroncite {{\v R}eh{\r u}{\v r}ek and Sojka}{2010}}
