\relax 
\citation{bouckaert2012mapping}
\citation{gray2009language}
\citation{dellert2015uralic}
\citation{bouchard2013automated}
\citation{rama2016siamese}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{3}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Motivation}{4}}
\newlabel{Motivation}{{2}{4}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Visualization of the concept of sound change as a walk in latent space. Here, sound changes are vectors from one word to another, where both word forms are given as points in latent space. The vector from /lob/ to /lop/ would be the same as from /tag/ to /tak/, as both vectors would describe the loss of voice in the word-final phoneme. This linear dependence means that if we fit a regression model from /tag/ to /tak/, we could generalize well to predict /lop/ from /lob/. The different lengths of the vectors are than proportional to probabilities of such as sound change to appear. Here, final devoicing should be more probable than a change from /lop/ to /tak/. \relax }}{4}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:final_devoicing}{{1}{4}}
\citation{bouchard2007probabilistic,bouchard2013automated}
\citation{bouchard2007probabilistic}
\newlabel{eq:final_devoicing}{{3}{5}}
\newlabel{eq:sound_change_linear_dependency}{{4}{5}}
\citation{mikolov2013distributed}
\citation{bergsma2007alignment,inkpen2005automatic}
\citation{kondrak2000new}
\citation{jager2014phylogenetic}
\citation{rama2016siamese}
\citation{list2012lexstat}
\@writefile{toc}{\contentsline {section}{\numberline {3}Related Research}{6}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Architecture}{6}}
\citation{bouchard2007probabilistic,bouchard2013automated}
\citation{Goodfellow-et-al-2016-Book}
\citation{chomsky1968sound}
\citation{dolgopolsky1986probabilistic}
\citation{list2012lexstat}
\citation{baxter2000beyond,mortarino2009improved,turchin2010analyzing}
\citation{list2012sca,list2012lexstat}
\citation{kondrak2000new}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Phoneme Vectorization}{7}}
\newlabel{Phoneme Vectorization}{{4.1}{7}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.1}Hand-crafted Vectorization Models}{7}}
\citation{rama2016siamese}
\citation{wichmann2010asjp}
\citation{rama2016siamese}
\citation{rama2016siamese}
\citation{landauer2013handbook}
\citation{mikolov2013efficient,mikolov2013distributed,goldberg2014word2vec}
\citation{pennington2014glove}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.2}Phoneme embeddings}{8}}
\newlabel{Phoneme embeddings}{{4.1.2}{8}}
\citation{morin2005hierarchical}
\citation{goldberg2014word2vec}
\citation{gutmann2010noise,mnih2012fast}
\newlabel{eq:hierarchicalsoftmax_nodeprob}{{8}{9}}
\newlabel{eq:neg_sampling_3}{{9}{9}}
\citation{jager2014phylogenetic,rama2016siamese}
\citation{mikolov2013efficient}
\citation{mikolov2013efficient}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces The two common architectures of word2vec. The Continuous Bag-of-Words (CBOW) model predicts the current word based on the context, and the Skip-gram predicts surrounding words given the current word. From \cite  {mikolov2013efficient}.\relax }}{10}}
\newlabel{fig:word2vec}{{2}{10}}
\citation{hinton2006reducing}
\citation{lauly2014autoencoder,zhang2014bilingually}
\citation{socher2011dynamic}
\citation{silberer2014learning,le2014distributed}
\citation{mikolov2013efficient,mikolov2013distributed,goldberg2014word2vec}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Some t-SNE visualizations of the embeddings created by word2vec. (top left) The model learns to clearly separate natural classes such as vowels, plain pulmonic or glottalized consonants, while other articulations seem to spread over the feature space. The colors indicate membership of a natural phonological class. (top right) A more detailed view on plain pulmonic consonants. Note the linear dependencies between voiced and unvoiced plosives and their respective nasal variant. (bottom left) Another detailed view. Note how the labialized uvular sounds cluster among glottalized consonants. (top right) The model seems to capture different manners of articulations across articulation type boundaries, as the linear dependency shows here.\relax }}{11}}
\newlabel{fig:phoneme_embeddings_word2vec_all}{{3}{11}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Word Embeddings}{11}}
\newlabel{Word Embeddings}{{4.2}{11}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.1}Autoencoders}{11}}
\citation{doersch2016tutorial}
\citation{doersch2016tutorial}
\citation{kingma2013auto}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Comparison of the four word2vec models evaluated. (top left) Both CBOW Models perform better than the skip-gram models over all numbers of embedding dimension. (bottom left) Mean distance between the predicted vector and the target with regard to embedding dimensions. Here, negative sampling yields less error than hierarchical softmax. (top right) Mean accuracy for the models with regard to the context window size. The models perform worse the bigger the context is. (bottom right) Mean distance between the predicted vector and the target with regard to context window size. Again, bigger contexts lead to worse predictions. \relax }}{12}}
\newlabel{fig:phoneme_embeddings_4models_comparison_meanAccLossTopn1}{{4}{12}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Mean accuracies for analogies. The model does not seem to be able to capture the compositionality of the latent features too well. (top left) Accuracy is best for plosives across plain pulmonic as well as more complex articulations, while fricatives perform worse. (top right) Glottalized and Plain pulmonic consonant phonemes yield best performance. Among complex articulations, which all perform bad, labialized phonemes yield best results, while aspirated and palatalized phonemes perform even worse. (bottom left) Adding voice works better than removing it or adding nasality. (bottom right) Vowel analogies work far better than consonant analogies. This should be due to the small number of possible vowel phonemes.\relax }}{13}}
\newlabel{fig:phoneme_embeddings_detailed_evaluation_topn1}{{5}{13}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Mean accuracies for analogies over the pooled consonant phonemes. Here, the model is expected to yield better results, as the number of latent features should have been reduced. (left) Accuracy is doubled compared with the unpooled phonemes. (middle) Adding voice works quite well, while removing it still yields acceptable performance. Applying nasality again works quite bad. (right) Vowel analogy tasks seem to work reasonably well, but worse than with unpooled consonants.\relax }}{13}}
\newlabel{fig:phoneme_embeddings_detailed_evaluation_topn1_poooled}{{6}{13}}
\citation{geman1984stochastic}
\citation{duane1987hybrid}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Visualization of the Variational autoencoder architecture. (left) The model with original objective as in Eq. 15\hbox {}. The stochastic unit is inside the network and would not allow for the backpropagation of error gradients through the network. (right) The model after the reparameterization with the objective as in Eq. 23\hbox {}. Here, the sampling is interpreted as an input variable, so error gradients can be backpropagated through the whole model. From \cite  {doersch2016tutorial}.\relax }}{14}}
\newlabel{fig:vae_model_visualization}{{7}{14}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.2}Variational Autoencoders}{14}}
\newlabel{Variational Autoencoders}{{4.2.2}{14}}
\citation{kingma2013auto}
\citation{kulkarni2015deep,bowman2015generating,fabius2014variational}
\newlabel{eq:vae_loss_original}{{15}{15}}
\newlabel{eq:multivariate_bernoulli_prob}{{17}{15}}
\citation{kingma2013auto}
\citation{kingma2013auto}
\newlabel{eq:vae_loss_reparameterized}{{23}{16}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.3}Interpretation of $\mathcal  {Z}$}{16}}
\citation{frey2007clustering}
\citation{wichmann2010asjp}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Clustering}{17}}
\newlabel{Clustering}{{4.3}{17}}
\citation{dunn2012indo}
\@writefile{toc}{\contentsline {section}{\numberline {5}Evaluation}{18}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Data}{18}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Visual Inspection}{18}}
\citation{dolgopolsky1986probabilistic,list2012sca}
\citation{list2012sca}
\citation{rama2016siamese}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces A detailed view on a subsample of ASJP embedded into $\mathcal  {Z}$ after trained on the whole ASJP data set for 100 iterations. As can be seen, words accumulate in local subspaces with higher probability mass. Here, all words are contained in an area with shorter words that show simple CV syllable structures. Words that only differ in one distinctive feature are very close to each other, while linear dependencies over longer distances signify relationships over whole syllables that are exchanged.\relax }}{19}}
\newlabel{fig:vae_phono_binary_asjp_subsample_words}{{8}{19}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3}Grid Search over Model Parameters}{19}}
\citation{rand1971objective,hubert1985comparing}
\citation{vinh2010information}
\citation{rosenberg2007v}
\newlabel{fig:vae_phono_binary_wordLength0}{{\caption@xref {fig:vae_phono_binary_wordLength0}{ on input line 461}}{20}}
\newlabel{sub@fig:vae_phono_binary_wordLength0}{{}{20}}
\newlabel{fig:vae_phono_binary_wordLength1}{{\caption@xref {fig:vae_phono_binary_wordLength1}{ on input line 467}}{20}}
\newlabel{sub@fig:vae_phono_binary_wordLength1}{{}{20}}
\newlabel{fig:vae_phono_binary_wordLength2}{{\caption@xref {fig:vae_phono_binary_wordLength2}{ on input line 473}}{20}}
\newlabel{sub@fig:vae_phono_binary_wordLength2}{{}{20}}
\newlabel{fig:vae_phono_binary_wordLength3}{{\caption@xref {fig:vae_phono_binary_wordLength3}{ on input line 478}}{20}}
\newlabel{sub@fig:vae_phono_binary_wordLength3}{{}{20}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces The ASJP data set embedded into $\mathcal  {Z}$ after trained on it for 100 iterations, colored by word length. The model learns to cluster words according to their respective length, with more frequent word lengths located in subspaces with higher probability mass.\relax }}{20}}
\newlabel{fig:vae_phono_binary_wordLength}{{9}{20}}
\citation{rehurek_lrec}
\citation{chollet2015keras}
\citation{tensorflow2015-whitepaper}
\citation{2016arXiv160502688short}
\citation{List2016e}
\citation{scikit-learn}
\newlabel{table:model_comparison}{{\caption@xref {table:model_comparison}{ on input line 612}}{21}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces The performance of VAE-C compared to LexStat. NLD-AP uses Normalized Levenshtein Distances as distance metric and Affinity Propagation as clustering algorithm. VAE-C performs only mediocre, but at least better than random labeling.\relax }}{21}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.4}Performance on IELex}{21}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.5}Discussion}{21}}
\@writefile{toc}{\contentsline {section}{\numberline {6}Resume}{21}}
\bibstyle{apacite}
\bibdata{references}
\bibcite{tensorflow2015-whitepaper}{{1}{{\APACyear {2015}}}{{Abadi\ \BOthers {.}}}{{Abadi\ \BOthers {.}}}}
\bibcite{baxter2000beyond}{{2}{{\APACyear {2000}}}{{Baxter\ \&{} Ramer}}{{Baxter\ \&{} Ramer}}}
\bibcite{Goodfellow-et-al-2016-Book}{{3}{{\APACyear {2016}}}{{Bengio\ \&{} Courville}}{{Bengio\ \&{} Courville}}}
\bibcite{bergsma2007alignment}{{4}{{\APACyear {2007}}}{{Bergsma\ \&{} Kondrak}}{{Bergsma\ \&{} Kondrak}}}
\bibcite{bouchard2013automated}{{5}{{\APACyear {2013}}}{{Bouchard-C{\^o}t{\'e}\ \BOthers {.}}}{{Bouchard-C{\^o}t{\'e}, Hall, Griffiths,{}\ \&{} Klein}}}
\bibcite{bouchard2007probabilistic}{{6}{{\APACyear {2007}}}{{Bouchard-C{\^o}t{\'e}\ \BOthers {.}}}{{Bouchard-C{\^o}t{\'e}, Liang, Griffiths,{}\ \&{} Klein}}}
\bibcite{bouckaert2012mapping}{{7}{{\APACyear {2012}}}{{Bouckaert\ \BOthers {.}}}{{Bouckaert\ \BOthers {.}}}}
\bibcite{bowman2015generating}{{8}{{\APACyear {2015}}}{{Bowman\ \BOthers {.}}}{{Bowman\ \BOthers {.}}}}
\bibcite{chollet2015keras}{{9}{{\APACyear {2015}}}{{Chollet}}{{Chollet}}}
\bibcite{chomsky1968sound}{{10}{{\APACyear {1968}}}{{Chomsky\ \&{} Halle}}{{Chomsky\ \&{} Halle}}}
\bibcite{dellert2015uralic}{{11}{{\APACyear {2015}}}{{Dellert}}{{Dellert}}}
\bibcite{doersch2016tutorial}{{12}{{\APACyear {2016}}}{{Doersch}}{{Doersch}}}
\bibcite{dolgopolsky1986probabilistic}{{13}{{\APACyear {1986}}}{{Dolgopolsky}}{{Dolgopolsky}}}
\bibcite{duane1987hybrid}{{14}{{\APACyear {1987}}}{{Duane\ \BOthers {.}}}{{Duane, Kennedy, Pendleton,{}\ \&{} Roweth}}}
\@writefile{toc}{\contentsline {section}{\numberline {7}Acknowledgements}{22}}
\bibcite{dunn2012indo}{{15}{{\APACyear {2012}}}{{Dunn}}{{Dunn}}}
\bibcite{fabius2014variational}{{16}{{\APACyear {2014}}}{{Fabius\ \&{} van Amersfoort}}{{Fabius\ \&{} van Amersfoort}}}
\bibcite{frey2007clustering}{{17}{{\APACyear {2007}}}{{Frey\ \&{} Dueck}}{{Frey\ \&{} Dueck}}}
\bibcite{geman1984stochastic}{{18}{{\APACyear {1984}}}{{Geman\ \&{} Geman}}{{Geman\ \&{} Geman}}}
\bibcite{goldberg2014word2vec}{{19}{{\APACyear {2014}}}{{Goldberg\ \&{} Levy}}{{Goldberg\ \&{} Levy}}}
\bibcite{gray2009language}{{20}{{\APACyear {2009}}}{{Gray\ \BOthers {.}}}{{Gray, Drummond,{}\ \&{} Greenhill}}}
\bibcite{gutmann2010noise}{{21}{{\APACyear {2010}}}{{Gutmann\ \&{} Hyv{\"a}rinen}}{{Gutmann\ \&{} Hyv{\"a}rinen}}}
\bibcite{hinton2006reducing}{{22}{{\APACyear {2006}}}{{Hinton\ \&{} Salakhutdinov}}{{Hinton\ \&{} Salakhutdinov}}}
\bibcite{hubert1985comparing}{{23}{{\APACyear {1985}}}{{Hubert\ \&{} Arabie}}{{Hubert\ \&{} Arabie}}}
\bibcite{inkpen2005automatic}{{24}{{\APACyear {2005}}}{{Inkpen\ \BOthers {.}}}{{Inkpen, Frunza,{}\ \&{} Kondrak}}}
\bibcite{jager2014phylogenetic}{{25}{{\APACyear {2014}}}{{J{\"a}ger}}{{J{\"a}ger}}}
\bibcite{kingma2013auto}{{26}{{\APACyear {2013}}}{{Kingma\ \&{} Welling}}{{Kingma\ \&{} Welling}}}
\bibcite{kondrak2000new}{{27}{{\APACyear {2000}}}{{Kondrak}}{{Kondrak}}}
\bibcite{kulkarni2015deep}{{28}{{\APACyear {2015}}}{{Kulkarni\ \BOthers {.}}}{{Kulkarni, Whitney, Kohli,{}\ \&{} Tenenbaum}}}
\bibcite{landauer2013handbook}{{29}{{\APACyear {2013}}}{{Landauer\ \BOthers {.}}}{{Landauer, McNamara, Dennis,{}\ \&{} Kintsch}}}
\bibcite{lauly2014autoencoder}{{30}{{\APACyear {2014}}}{{Lauly\ \BOthers {.}}}{{Lauly\ \BOthers {.}}}}
\bibcite{le2014distributed}{{31}{{\APACyear {2014}}}{{Le\ \&{} Mikolov}}{{Le\ \&{} Mikolov}}}
\bibcite{list2012lexstat}{{32}{{\APACyear {2012}}{\APACexlab {{\BCnt {1}}}}}{{List}}{{List}}}
\bibcite{list2012sca}{{33}{{\APACyear {2012}}{\APACexlab {{\BCnt {2}}}}}{{List}}{{List}}}
\bibcite{List2016e}{{34}{{\APACyear {2016}}}{{List\ \&{} Forkel}}{{List\ \&{} Forkel}}}
\bibcite{mikolov2013efficient}{{35}{{\APACyear {2013}}}{{Mikolov, Chen,{}\ \BOthers {.}}}{{Mikolov, Chen, Corrado,{}\ \&{} Dean}}}
\bibcite{mikolov2013distributed}{{36}{{\APACyear {2013}}}{{Mikolov, Sutskever,{}\ \BOthers {.}}}{{Mikolov, Sutskever, Chen, Corrado,{}\ \&{} Dean}}}
\bibcite{mnih2012fast}{{37}{{\APACyear {2012}}}{{Mnih\ \&{} Teh}}{{Mnih\ \&{} Teh}}}
\bibcite{morin2005hierarchical}{{38}{{\APACyear {2005}}}{{Morin\ \&{} Bengio}}{{Morin\ \&{} Bengio}}}
\bibcite{mortarino2009improved}{{39}{{\APACyear {2009}}}{{Mortarino}}{{Mortarino}}}
\bibcite{scikit-learn}{{40}{{\APACyear {2011}}}{{Pedregosa\ \BOthers {.}}}{{Pedregosa\ \BOthers {.}}}}
\bibcite{pennington2014glove}{{41}{{\APACyear {2014}}}{{Pennington\ \BOthers {.}}}{{Pennington, Socher,{}\ \&{} Manning}}}
\bibcite{rama2016siamese}{{42}{{\APACyear {2016}}}{{Rama}}{{Rama}}}
\bibcite{rand1971objective}{{43}{{\APACyear {1971}}}{{Rand}}{{Rand}}}
\bibcite{rehurek_lrec}{{44}{{\APACyear {2010}}}{{{\v R}eh{\r u}{\v r}ek\ \&{} Sojka}}{{{\v R}eh{\r u}{\v r}ek\ \&{} Sojka}}}
\bibcite{rosenberg2007v}{{45}{{\APACyear {2007}}}{{Rosenberg\ \&{} Hirschberg}}{{Rosenberg\ \&{} Hirschberg}}}
\bibcite{silberer2014learning}{{46}{{\APACyear {2014}}}{{Silberer\ \&{} Lapata}}{{Silberer\ \&{} Lapata}}}
\bibcite{socher2011dynamic}{{47}{{\APACyear {2011}}}{{Socher\ \BOthers {.}}}{{Socher, Huang, Pennin, Manning,{}\ \&{} Ng}}}
\bibcite{sokal1958statistical}{{48}{{\APACyear {1958}}}{{Sokal}}{{Sokal}}}
\bibcite{2016arXiv160502688short}{{49}{{\APACyear {2016}}}{{{Theano Development Team}}}{{{Theano Development Team}}}}
\bibcite{turchin2010analyzing}{{50}{{\APACyear {2010}}}{{Turchin\ \BOthers {.}}}{{Turchin, Peiros,{}\ \&{} Gell-Mann}}}
\bibcite{vinh2010information}{{51}{{\APACyear {2010}}}{{Vinh\ \BOthers {.}}}{{Vinh, Epps,{}\ \&{} Bailey}}}
\bibcite{wichmann2010asjp}{{52}{{\APACyear {2010}}}{{Wichmann\ \BOthers {.}}}{{Wichmann\ \BOthers {.}}}}
\bibcite{zhang2014bilingually}{{53}{{\APACyear {2014}}}{{Zhang\ \BOthers {.}}}{{Zhang\ \BOthers {.}}}}
\newlabel{fig:vae_phono_binary_features0}{{\caption@xref {fig:vae_phono_binary_features0}{ on input line 488}}{26}}
\newlabel{sub@fig:vae_phono_binary_features0}{{}{26}}
\newlabel{fig:vae_phono_binary_features1}{{\caption@xref {fig:vae_phono_binary_features1}{ on input line 494}}{26}}
\newlabel{sub@fig:vae_phono_binary_features1}{{}{26}}
\newlabel{fig:vae_phono_binary_features2}{{\caption@xref {fig:vae_phono_binary_features2}{ on input line 500}}{26}}
\newlabel{sub@fig:vae_phono_binary_features2}{{}{26}}
\newlabel{fig:vae_phono_binary_feature3}{{\caption@xref {fig:vae_phono_binary_feature3}{ on input line 505}}{26}}
\newlabel{sub@fig:vae_phono_binary_feature3}{{}{26}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces The ASJP data set embedded into $\mathcal  {Z}$ after trained on it for 100 iterations, colored by whether a given word has a phoneme with some specific distinctive feature. (top left) The model learns that clicks are highly unlikely to emerge evolutionary and hence assigns low probability mass to their respective subspace. (top right) The distribution of words with and without dentals. The linear dependencies are clearly visible. (bottom left) The distribution of words with and without fricatives. Again, it can be seen how they are linearly dependent of each other. (bottom right) A detailed view on the words with and without fricatives.\relax }}{26}}
\newlabel{fig:vae_phono_binary_features}{{10}{26}}
\newlabel{fig:vae_phono_orthofeatures0}{{\caption@xref {fig:vae_phono_orthofeatures0}{ on input line 515}}{27}}
\newlabel{sub@fig:vae_phono_orthofeatures0}{{}{27}}
\newlabel{fig:vae_phono_orthofeatures1}{{\caption@xref {fig:vae_phono_orthofeatures1}{ on input line 521}}{27}}
\newlabel{sub@fig:vae_phono_orthofeatures1}{{}{27}}
\newlabel{fig:vae_phono_orthofeatures2}{{\caption@xref {fig:vae_phono_orthofeatures2}{ on input line 527}}{27}}
\newlabel{sub@fig:vae_phono_orthofeatures2}{{}{27}}
\newlabel{fig:vae_phono_orthofeatures3}{{\caption@xref {fig:vae_phono_orthofeatures3}{ on input line 532}}{27}}
\newlabel{sub@fig:vae_phono_orthofeatures3}{{}{27}}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces The ASJP data set embedded into $\mathcal  {Z}$ after trained on it for 100 iterations. Since the binary features ignore coarticulations as such and encode them as independent phonemes instead, the model might learn to distinguish them, given some underlying latent features. (top left) The distribution of words that contain plain /w/ versus words with proper labialization. If the model did not learn the difference, we would expect some uniform distributions over all words that contain /w/ in the input. However, we can clearly see that words with proper labialization are located in other subspaces than words with proper /w/. Moreover, we see that words with labialized phonemes show linear dependence. (top right) The distribution of words that contain plain /y/ versus words with proper palatalization. Again, we see a clear distinction between the two. (bottom left) The distribution of words that contain plain /h/ versus words with proper aspiration. Again, we see a clear distinction between the two. (bottom right) The distribution of words that contain glottalized sounds versus that of words without. As glottalization can cover both vowels and consonants, the distributions are spread over the whole population. However, we can see that glottalized sounds cluster at the bottom right, a subspace containing mostly mostly words with velar and uvular consonants.\relax }}{27}}
\newlabel{fig:vae_phono_orthofeatures}{{11}{27}}
\newlabel{fig:vae_phono_orthofeatures0}{{\caption@xref {fig:vae_phono_orthofeatures0}{ on input line 543}}{28}}
\newlabel{sub@fig:vae_phono_orthofeatures0}{{}{28}}
\newlabel{fig:vae_phono_orthofeatures1}{{\caption@xref {fig:vae_phono_orthofeatures1}{ on input line 549}}{28}}
\newlabel{sub@fig:vae_phono_orthofeatures1}{{}{28}}
\newlabel{fig:vae_phono_orthofeatures2}{{\caption@xref {fig:vae_phono_orthofeatures2}{ on input line 555}}{28}}
\newlabel{sub@fig:vae_phono_orthofeatures2}{{}{28}}
\newlabel{fig:vae_phono_orthofeatures3}{{\caption@xref {fig:vae_phono_orthofeatures3}{ on input line 560}}{28}}
\newlabel{sub@fig:vae_phono_orthofeatures3}{{}{28}}
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces The posterior distribution $P(z|X)$ with X being the IELex data with regards to the single semantic concepts. The model was trained on ASJP for 2000 iterations. Colors indicate the respective true cognate class. (top left) The posterior for the concept "name". As all words are cognates to each other, we see strong linear dependencies over the distribution. Also note that the embeddings are embedded into a latent continuum of word lengths, from the the top left to the bottom right. Moreover, the members of the left hand side clusters all start with consonants, while the members of the clusters on right hand side all start with vowels or approximants. (top right) The posterior for "sing". Again, cognates show stronger linear dependencies among each other. (bottom left) The posterior for "snake". The romance words (pink samples) show linear dependencies. (bottom right) A closer look on some words for "snake". As VAE-PT concentrates on clustering words with similar syllable structures, it tends to cluster words that share such similarities close to each other. Here, it overestimates the similarity between Slovak /had/ and Upper Sorbian /huZ/ on the one side and the words /sap/ (Bihari,Magahi,Urdu,Marathi) and /xap/ (Assamese) on the other. \relax }}{28}}
\newlabel{fig:asjp_2000_binary_singleIELEXConcepts_posteriors}{{12}{28}}
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces VAE-C trained on the IELex data for "snake". The left plot shows true cognates classes, the right plot shows inferred labels. Cognates, as long as they appear similar, are clustered at some subspace of $\mathcal  {Z}$. Words with low self-information, such was words with no inferred cognates, are seen as "noise" by the model and grouped close to each other at the center of the latent space.\relax }}{29}}
\newlabel{fig:vae_c_snake}{{13}{29}}
\@writefile{lof}{\contentsline {figure}{\numberline {14}{\ignorespaces The result of the grid search over several parameters of VAE-C, measured in adjusted rand indices. In general, the performance is mediocre. (top left) The binary phoneme features outperform the phoneme embeddings. (top right) 20 latent dimensions seem to outperform the other parameters. (bottom left) As the number of intermediate dimensions of the encoder and decoder MLPs increases, the performance also improves slightly. (bottom left) The results suggest that a smaller standard deviation of the auxiliary noise improves the performance.\relax }}{30}}
\newlabel{fig:grid_search_boxplots}{{14}{30}}
\@writefile{lof}{\contentsline {figure}{\numberline {15}{\ignorespaces The performance of VAE-C on IELex, using the parameters obtained by the grid search (left), and random labeling (right). The measures are adjusted rand indices, adjusted mutual information, cluster homogeneity, cluster completeness and v-measure, as the harmonic mean of the latter two. The model learns to cluster similar words together, although the overall performance is mediocre.\relax }}{31}}
\newlabel{fig:boxplots_IELex}{{15}{31}}
